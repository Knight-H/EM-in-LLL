{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b8b70c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import model_classes\n",
    "\n",
    "import pytorch_transformers\n",
    "from transformers import BertTokenizer # Use the new BertTokenizer for batch_encode_plus!\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import torchtext\n",
    "from utils import dynamic_collate_fn, prepare_inputs, read_relations, read_rel_data, \\\n",
    "                    get_relation_embedding, prepare_rel_datasets, rel_encode, replicate_rel_data, \\\n",
    "                    get_relation_index, create_relation_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a3b5e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7572f469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a17bb1416fd487dbac6f99fe1d19530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d051956b834468ba2fbfd59077fcb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8764e0c2ad554fbbaddafe5f41ba2ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config_class, model_class, tokenizer_class = model_classes[\"bert\"]\n",
    "tokenizer = tokenizer_class.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer2 = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65ebc63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading the dataset\n",
      "Loading GloVe vectors\n",
      "Finished loading GloVe vectors\n",
      "torch.Size([81, 300])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/LAMOL/github/LAMOL/env/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "data_dir = '/data/omler_data/LifelongFewRel'\n",
    "relation_file = os.path.join(data_dir, 'relation_name.txt')\n",
    "training_file = os.path.join(data_dir, 'training_data.txt')\n",
    "validation_file = os.path.join(data_dir, 'val_data.txt')\n",
    "relation_names = read_relations(relation_file) # List of relation names (converted to 1-based index later)\n",
    "train_data = read_rel_data(training_file)\n",
    "val_data = read_rel_data(validation_file)\n",
    "print('Finished loading the dataset')\n",
    "# Load GloVe vectors\n",
    "print('Loading GloVe vectors')\n",
    "glove = torchtext.vocab.GloVe(name='6B', dim=300)\n",
    "print('Finished loading GloVe vectors')\n",
    "# Get relation embeddings for clustering\n",
    "relation_embeddings = get_relation_embedding(relation_names, glove)\n",
    "print(relation_embeddings.shape)\n",
    "\n",
    "# Generate clusters\n",
    "# This essentially goes through all train_data and get label set, which is a list of 1-80 ie. [80, 25, 75, 15, 62, 74, 5, 10...] \n",
    "relation_index = get_relation_index(train_data)  \n",
    "# This uses KMeans to divide the label up into 10 disjoint clusters ie. {80: 1, 25: 5, 75: 3, 15: 1, 62: 1, 74: 1, 5: 1, 10: 2...}\n",
    "# > relation_embeddings just return a dictionary of relation_index --> Glove embedding ie. { 80: embedding, 25: embedding, ...}\n",
    "cluster_labels, relation_embeddings = create_relation_clusters(10, relation_embeddings, relation_index)\n",
    "train_datasets, shuffle_index = prepare_rel_datasets(train_data, relation_names, cluster_labels, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82487333",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_datasets[0]\n",
    "train_dataloader = DataLoader(train_dataset, num_workers=1, batch_size=4, shuffle=False, collate_fn=rel_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57d02537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['in',\n",
       "  '2004',\n",
       "  'the',\n",
       "  'catalan',\n",
       "  'government',\n",
       "  'gave',\n",
       "  'him',\n",
       "  'the',\n",
       "  'george',\n",
       "  'cross',\n",
       "  '.'],\n",
       " ['applies', 'to', 'jurisdiction'],\n",
       " [['military', 'branch']])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c9cb35e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replicated_text [['in', '2004', 'the', 'catalan', 'government', 'gave', 'him', 'the', 'george', 'cross', '.'], ['in', '2004', 'the', 'catalan', 'government', 'gave', 'him', 'the', 'george', 'cross', '.'], ['\"', 'a', 'twentieth', 'century', 'history', 'of', 'berrien', 'county', 'michigan', '\"', ',', 'p.', '262', '.'], ['\"', 'a', 'twentieth', 'century', 'history', 'of', 'berrien', 'county', 'michigan', '\"', ',', 'p.', '262', '.'], ['this', 'bridge', 'is', 'located', 'in', 'the', 'himalayan', 'mountains', 'between', 'the', 'dras', 'river', 'and', 'suru', 'river', 'in', 'the', 'ladakh', 'valley', 'in', 'the', 'indian', 'state', 'of', 'jammu', 'and', 'kashmir', '.'], ['this', 'bridge', 'is', 'located', 'in', 'the', 'himalayan', 'mountains', 'between', 'the', 'dras', 'river', 'and', 'suru', 'river', 'in', 'the', 'ladakh', 'valley', 'in', 'the', 'indian', 'state', 'of', 'jammu', 'and', 'kashmir', '.'], ['1997', ':', 'wales', 'voted', 'in', 'favour', 'of', 'a', 'welsh', 'assembly', 'in', 'a', 'national', 'referendum', ',', 'but', 'cardiff', 'again', 'voted', 'against', 'it', '.'], ['1997', ':', 'wales', 'voted', 'in', 'favour', 'of', 'a', 'welsh', 'assembly', 'in', 'a', 'national', 'referendum', ',', 'but', 'cardiff', 'again', 'voted', 'against', 'it', '.']]\n",
      "replicated_relations [['applies', 'to', 'jurisdiction'], ['military', 'branch'], ['contains', 'administrative', 'territorial', 'entity'], ['heritage', 'designation'], ['located', 'in', 'the', 'administrative', 'territorial', 'entity'], ['military', 'branch'], ['applies', 'to', 'jurisdiction'], ['military', 'rank']]\n",
      "ranking_label [1, 0, 1, 0, 1, 0, 1, 0]\n",
      "{'input_ids': tensor([[  101,  1999,  2432,  1996, 13973,  2231,  2435,  2032,  1996,  2577,\n",
      "          2892,  1012,   102, 12033,  2000,  7360,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1999,  2432,  1996, 13973,  2231,  2435,  2032,  1996,  2577,\n",
      "          2892,  1012,   102,  2510,  3589,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1000,  1037,  9086,  2301,  2381,  1997,   100,  2221,  4174,\n",
      "          1000,  1010,   100, 21950,  1012,   102,  3397,  3831,  7894,  9178,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1000,  1037,  9086,  2301,  2381,  1997,   100,  2221,  4174,\n",
      "          1000,  1010,   100, 21950,  1012,   102,  4348,  8259,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2023,  2958,  2003,  2284,  1999,  1996, 28333,  4020,  2090,\n",
      "          1996,   100,  2314,  1998,   100,  2314,  1999,  1996,   100,  3028,\n",
      "          1999,  1996,  2796,  2110,  1997, 21433,  1998, 13329,  1012,   102,\n",
      "          2284,  1999,  1996,  3831,  7894,  9178,   102],\n",
      "        [  101,  2023,  2958,  2003,  2284,  1999,  1996, 28333,  4020,  2090,\n",
      "          1996,   100,  2314,  1998,   100,  2314,  1999,  1996,   100,  3028,\n",
      "          1999,  1996,  2796,  2110,  1997, 21433,  1998, 13329,  1012,   102,\n",
      "          2510,  3589,   102,     0,     0,     0,     0],\n",
      "        [  101,  2722,  1024,  3575,  5444,  1999,  7927,  1997,  1037,  6124,\n",
      "          3320,  1999,  1037,  2120,  9782,  1010,  2021, 10149,  2153,  5444,\n",
      "          2114,  2009,  1012,   102, 12033,  2000,  7360,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2722,  1024,  3575,  5444,  1999,  7927,  1997,  1037,  6124,\n",
      "          3320,  1999,  1037,  2120,  9782,  1010,  2021, 10149,  2153,  5444,\n",
      "          2114,  2009,  1012,   102,  2510,  4635,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "8\n",
      "37\n",
      "['[CLS]', 'in', '2004', 'the', 'catalan', 'government', 'gave', 'him', 'the', 'george', 'cross', '.', '[SEP]', 'applies', 'to', 'jurisdiction', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(train_dataloader):\n",
    "    text, labels, candidates = batch\n",
    "    replicated_text, replicated_relations, ranking_label = replicate_rel_data(text,labels,candidates)\n",
    "    print(\"replicated_text\", replicated_text)\n",
    "    print(\"replicated_relations\", replicated_relations)\n",
    "    print(\"ranking_label\", ranking_label)\n",
    "    output = tokenizer2.batch_encode_plus(list(zip(replicated_text, replicated_relations)), return_token_type_ids=False, \n",
    "                                          padding='longest', return_tensors='pt')\n",
    "    print(output)\n",
    "    print(len(output['input_ids']))\n",
    "    print(len(output['input_ids'][0]))\n",
    "    print([tokenizer2.convert_ids_to_tokens(x) for x in output['input_ids'].tolist()[0]])\n",
    "    \n",
    "#     for t in zip(replicated_text, replicated_relations):\n",
    "#         print(t[0])\n",
    "#         output_id = tokenizer2.encode_plus(list(t), return_token_type_ids=False, \\\n",
    "#                             truncation=True, padding='max_length', return_tensors='pt')\n",
    "#         print(output_id)\n",
    "#     print(replicated_text)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9ee338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea3a0f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3231]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "030e3add",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BertTokenizer' object has no attribute 'batch_encode_plus'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BertTokenizer' object has no attribute 'batch_encode_plus'"
     ]
    }
   ],
   "source": [
    "tokenizer.batch_encode_plus(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cd93f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2224f451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Test', 'aspok'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([\"Test\", \"aspok\"], dtype=object)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lamol",
   "language": "python",
   "name": "lamol"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
