{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b432e95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_transformers import AdamW, WarmupLinearSchedule, WarmupConstantSchedule\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import argparse\n",
    "import copy\n",
    "import logging\n",
    "import numpy as np\n",
    "import os, time, csv, random\n",
    "import pickle\n",
    "import higher\n",
    "from settings import parse_test_args, model_classes, init_logging\n",
    "from utils import TextClassificationDataset, dynamic_collate_fn, prepare_inputs, DynamicBatchSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18ecb72",
   "metadata": {},
   "source": [
    "# From Meta-MbPA Paper\n",
    "\n",
    ">In our experiments, we find that it is sufficient to take this to the extreme such that we consider all test examples as a single cluster. Consequently, we consider the whole memory as neighbours and we randomly sample from it to be comparable with the original local adaptation for- mulation (i.e. same batch sizes and gradient steps). As shown in the next section, it has two benefits: (1) it is more robust to negative transfer, (2) it is faster when we evaluate testing examples as a group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0b9e285",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0d70274",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dotdict({\n",
    "#     \"output_dir\": \"/data/model_runs/em_in_lll/MetaMBPA_order1\",\n",
    "#     \"output_dir\": \"/data/model_runs/em_in_lll/MetaMBPA_order1_v2\",\n",
    "#     \"output_dir\": \"/data/model_runs/em_in_lll/MetaMBPA_order1_v3\",\n",
    "#     \"output_dir\": \"/data/model_runs/em_in_lll/MetaMBPA_order1_v32\",\n",
    "#     \"output_dir\": \"/data/model_runs/em_in_lll/MetaMBPA_order2_v3\",\n",
    "#     \"output_dir\": \"/data/model_runs/em_in_lll/MetaMBPA_order3_v3\",\n",
    "    \"output_dir\": \"/data/model_runs/em_in_lll/MetaMBPA_order4_v3\",\n",
    "    \"adapt_lambda\": 1e-3,\n",
    "    \"adapt_lr\": 5e-5,\n",
    "    \"adapt_steps\": 30,\n",
    "    \"no_fp16_test\": False,\n",
    "    \"seed\": 42\n",
    "})\n",
    "output_dir = args[\"output_dir\"]\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aaf46c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output_dir': '/data/model_runs/em_in_lll/MetaMBPA_order4_v3', 'adapt_lambda': 0.001, 'adapt_lr': 5e-05, 'adapt_steps': 30, 'no_fp16_test': False, 'seed': 42, 'adam_epsilon': 1e-08, 'batch_size': 12353, 'debug': False, 'learning_rate': 3e-05, 'logging_steps': 500, 'max_grad_norm': 1.0, 'model_name': 'bert-base-uncased', 'model_type': 'bert', 'n_labels': 33, 'n_neighbors': 32, 'n_test': 7600, 'n_train': 115000, 'n_workers': 4, 'overwrite': False, 'replay_interval': 100, 'reproduce': False, 'tasks': ['ag_news_csv', 'yelp_review_full_csv', 'amazon_review_full_csv', 'yahoo_answers_csv', 'dbpedia_csv'], 'valid_ratio': 0, 'warmup_steps': 0, 'weight_decay': 0, 'inner_lr': 1e-05, 'write_prob': 0.01, 'device_id': 0}\n"
     ]
    }
   ],
   "source": [
    "train_args = pickle.load(open(os.path.join(output_dir, 'train_args'), 'rb'))\n",
    "args.update(train_args.__dict__)\n",
    "print(str(args))\n",
    "model_type = args[\"model_type\"]\n",
    "model_name = args[\"model_name\"]\n",
    "n_labels = args[\"n_labels\"]\n",
    "tasks = args[\"tasks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15db9f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_task(task_id, args, model, test_dataset):\n",
    "\n",
    "    if not args.no_fp16_test:\n",
    "        model = model.half()\n",
    "\n",
    "    def update_metrics(loss, logits, cur_loss, cur_acc):\n",
    "        preds = np.argmax(logits, axis=1)\n",
    "        return cur_loss + loss, cur_acc + np.sum(preds == labels.detach().cpu().numpy())\n",
    "    \n",
    "    # Before anything else, just sample randomly from memory!\n",
    "    # Use this as sample going forward!\n",
    "    s_input_ids, s_masks, s_labels = memory.sample(32)\n",
    "    print(f\"Total No.# of sampled: {len(s_labels)}\")\n",
    "    # query like this first just like training... this will need to be removed later!!! so we can adapt 32x32\n",
    "    with torch.no_grad():\n",
    "        q_input_ids, q_masks, q_labels = memory.query(s_input_ids, s_masks)\n",
    "        # Get only one is enough! (Batch of 32)\n",
    "        # Note: Need to find way to improve later\n",
    "        q_input_ids = q_input_ids[0].cuda()\n",
    "        q_masks     = q_masks[0].cuda()\n",
    "        q_labels    = q_labels[0].cuda()\n",
    "    \n",
    "    # Meta-Learning Local Adaptation\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    inner_optimizer = optim.SGD(optimizer_grouped_parameters, lr=args.learning_rate)\n",
    "    \n",
    "    \n",
    "    # Get Original Params\n",
    "    with torch.no_grad():\n",
    "        org_params = torch.cat([torch.reshape(param, [-1]) for param in model.parameters()], 0)\n",
    "\n",
    "    with higher.innerloop_ctx(model, inner_optimizer, copy_initial_weights=False,track_higher_grads=False) as (fmodel, diffopt):\n",
    "        # 1 Inner Loop (Support Set) - Once for all testing samples\n",
    "        for step in range(args.adapt_steps):\n",
    "            params = torch.cat([torch.reshape(param, [-1]) for param in fmodel.parameters()], 0)\n",
    "            loss = fmodel(input_ids=q_input_ids, attention_mask=q_masks, labels=q_labels)[0]\\\n",
    "                    + args.adapt_lambda * torch.sum((org_params - params)**2)\n",
    "            diffopt.step(loss)\n",
    "            fmodel.zero_grad() # Is this necessary? but local adapt have this!\n",
    "            \n",
    "        # 2 Outer Loop (Query Set)\n",
    "        tot_n_inputs = 0\n",
    "        cur_loss, cur_acc = 0, 0\n",
    "        all_labels, all_label_confs, all_preds = [], [], []\n",
    "        \n",
    "        test_dataloader = DataLoader(test_dataset, num_workers=args.n_workers, collate_fn=dynamic_collate_fn,\n",
    "                                     batch_sampler=DynamicBatchSampler(test_dataset, args.batch_size * 4))\n",
    "\n",
    "        for step, batch in enumerate(test_dataloader):\n",
    "            n_inputs, input_ids, masks, labels = prepare_inputs(batch)\n",
    "            tot_n_inputs += n_inputs\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                fmodel.eval()\n",
    "                output = fmodel(input_ids=input_ids, attention_mask=masks, labels=labels)[:2]\n",
    "                loss = output[0].item()\n",
    "                logits = output[1].detach().cpu().numpy()\n",
    "                softmax = F.softmax(output[1], -1) \n",
    "                \n",
    "            # Output has size of torch.Size([16, 33]) [BATCH, CLASSES]\n",
    "            label_conf = softmax[np.arange(len(softmax)), labels] # Select labels in the softmax of 33 classes\n",
    "            preds = np.argmax(logits, axis=1)\n",
    "\n",
    "            cur_loss, cur_acc = update_metrics(loss*n_inputs, logits, cur_loss, cur_acc)\n",
    "\n",
    "            # Append all!\n",
    "            all_labels.extend(labels.tolist())\n",
    "            all_label_confs.extend(label_conf.tolist())\n",
    "            all_preds.extend(preds.tolist())\n",
    "\n",
    "            if (step+1) % args.logging_steps == 0:\n",
    "                print(\"Tested {}/{} examples, test loss: {:.3f} , test acc: {:.3f}\".format(\n",
    "                    tot_n_inputs, len(test_dataset), cur_loss/tot_n_inputs, cur_acc/tot_n_inputs))\n",
    "\n",
    "\n",
    "    print(\"test loss: {:.3f} , test acc: {:.3f}\".format(\n",
    "        cur_loss / len(test_dataset), cur_acc / len(test_dataset)))\n",
    "    return cur_acc / len(test_dataset), all_labels, all_label_confs, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9b4f8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start testing ag_news_csv...\n",
      "Total No.# of sampled: 32\n",
      "test loss: 0.194 , test acc: 0.938\n",
      "Start testing yelp_review_full_csv...\n",
      "Total No.# of sampled: 32\n",
      "test loss: 8.524 , test acc: 0.000\n",
      "Start testing amazon_review_full_csv...\n",
      "Total No.# of sampled: 32\n",
      "test loss: 8.370 , test acc: 0.000\n",
      "Start testing yahoo_answers_csv...\n",
      "Total No.# of sampled: 32\n",
      "test loss: 8.972 , test acc: 0.000\n",
      "Start testing dbpedia_csv...\n",
      "Total No.# of sampled: 32\n",
      "test loss: 6.947 , test acc: 0.000\n",
      "Average Accuracy: 0.18760526315789475\n",
      "Accuracies: [0.9380263157894737, 0.0, 0.0, 0.0, 0.0]\n",
      "Start testing ag_news_csv...\n",
      "Total No.# of sampled: 32\n",
      "test loss: 0.324 , test acc: 0.908\n",
      "Start testing yelp_review_full_csv...\n",
      "Total No.# of sampled: 32\n",
      "test loss: 0.945 , test acc: 0.602\n",
      "Start testing amazon_review_full_csv...\n",
      "Total No.# of sampled: 32\n",
      "test loss: 1.140 , test acc: 0.542\n",
      "Start testing yahoo_answers_csv...\n",
      "Total No.# of sampled: 32\n",
      "test loss: 8.098 , test acc: 0.000\n",
      "Start testing dbpedia_csv...\n",
      "Total No.# of sampled: 32\n",
      "test loss: 5.287 , test acc: 0.000\n",
      "Average Accuracy: 0.4104473684210526\n",
      "Accuracies: [0.9084210526315789, 0.6015789473684211, 0.5422368421052631, 0.0, 0.0]\n",
      "Start testing ag_news_csv...\n",
      "Total No.# of sampled: 32\n",
      "test loss: 0.410 , test acc: 0.893\n",
      "Start testing yelp_review_full_csv...\n",
      "Total No.# of sampled: 32\n",
      "test loss: 0.949 , test acc: 0.591\n",
      "Start testing amazon_review_full_csv...\n",
      "Total No.# of sampled: 32\n",
      "test loss: 0.923 , test acc: 0.612\n",
      "Start testing yahoo_answers_csv...\n",
      "Total No.# of sampled: 32\n",
      "test loss: 12.040 , test acc: 0.000\n",
      "Start testing dbpedia_csv...\n",
      "Total No.# of sampled: 32\n",
      "test loss: 6.395 , test acc: 0.000\n",
      "Average Accuracy: 0.41928947368421055\n",
      "Accuracies: [0.8928947368421053, 0.5911842105263158, 0.6123684210526316, 0.0, 0.0]\n",
      "Start testing ag_news_csv...\n",
      "Total No.# of sampled: 32\n",
      "test loss: 0.460 , test acc: 0.880\n",
      "Start testing yelp_review_full_csv...\n",
      "Total No.# of sampled: 32\n",
      "test loss: 1.229 , test acc: 0.548\n",
      "Start testing amazon_review_full_csv...\n",
      "Total No.# of sampled: 32\n",
      "test loss: 1.432 , test acc: 0.510\n",
      "Start testing yahoo_answers_csv...\n",
      "Total No.# of sampled: 32\n",
      "test loss: 0.865 , test acc: 0.728\n",
      "Start testing dbpedia_csv...\n",
      "Total No.# of sampled: 32\n",
      "test loss: 5.219 , test acc: 0.000\n",
      "Average Accuracy: 0.5333157894736842\n",
      "Accuracies: [0.8798684210526316, 0.5481578947368421, 0.5101315789473684, 0.728421052631579, 0.0]\n",
      "Start testing ag_news_csv...\n",
      "Total No.# of sampled: 32\n",
      "test loss: 0.454 , test acc: 0.880\n",
      "Start testing yelp_review_full_csv...\n",
      "Total No.# of sampled: 32\n",
      "test loss: 1.151 , test acc: 0.556\n",
      "Start testing amazon_review_full_csv...\n",
      "Total No.# of sampled: 32\n",
      "test loss: 1.238 , test acc: 0.526\n",
      "Start testing yahoo_answers_csv...\n",
      "Total No.# of sampled: 32\n",
      "test loss: 1.050 , test acc: 0.697\n",
      "Start testing dbpedia_csv...\n",
      "Total No.# of sampled: 32\n",
      "test loss: 0.035 , test acc: 0.992\n",
      "Average Accuracy: 0.730263157894737\n",
      "Accuracies: [0.8798684210526316, 0.5563157894736842, 0.5260526315789473, 0.6967105263157894, 0.9923684210526316]\n",
      "Time run in 467.73912286758423 seconds\n",
      "[0.18760526315789475, 0.4104473684210526, 0.41928947368421055, 0.5333157894736842, 0.730263157894737]\n"
     ]
    }
   ],
   "source": [
    "tic_RUN = time.time()\n",
    "\n",
    "all_avg_acc = []\n",
    "all_acc = []\n",
    "\n",
    "# Test for all sequential  tasks\n",
    "for run_task in range(0, len(tasks)):\n",
    "    config_class, model_class, args[\"tokenizer_class\"] = model_classes[model_type]\n",
    "    tokenizer = args[\"tokenizer_class\"].from_pretrained(model_name)\n",
    "    model_config = config_class.from_pretrained(model_name, num_labels=n_labels, hidden_dropout_prob=0, attention_probs_dropout_prob=0)\n",
    "    save_model_path = os.path.join(output_dir, f'checkpoint-{run_task}')\n",
    "    model = model_class.from_pretrained(save_model_path, config=model_config).cuda()\n",
    "    memory = pickle.load(open(os.path.join(output_dir, f'memory-{run_task}'), 'rb'))\n",
    "    \n",
    "    \n",
    "    avg_acc = 0\n",
    "    accuracies = []\n",
    "    data_for_visual = []\n",
    "    for task_id, task in enumerate(tasks):\n",
    "        print(\"Start testing {}...\".format(task))\n",
    "        test_dataset = TextClassificationDataset(task, \"test\", args, tokenizer)\n",
    "        task_acc, all_labels, all_label_confs, all_preds = test_task(task_id, args, model, test_dataset)\n",
    "\n",
    "        # Start Edit\n",
    "        data_ids = [task + str(i) for i in range(len(all_labels))]\n",
    "        data_for_visual.extend(list(zip(data_ids, all_labels, all_label_confs, all_preds)))\n",
    "        accuracies.append(task_acc)\n",
    "\n",
    "        avg_acc += task_acc / len(args.tasks)\n",
    "    print(f\"Average Accuracy: {avg_acc}\")\n",
    "    print(f\"Accuracies: {accuracies}\")\n",
    "    \n",
    "    all_avg_acc.append(avg_acc)\n",
    "    all_acc.append(accuracies)\n",
    "    \n",
    "toc_RUN = time.time() - tic_RUN\n",
    "print(f\"Time run in {toc_RUN} seconds\")\n",
    "print(all_avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49f711d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9380263157894737\t0.0\t0.0\t0.0\t0.0\n",
      "0.9084210526315789\t0.6015789473684211\t0.5422368421052631\t0.0\t0.0\n",
      "0.8928947368421053\t0.5911842105263158\t0.6123684210526316\t0.0\t0.0\n",
      "0.8798684210526316\t0.5481578947368421\t0.5101315789473684\t0.728421052631579\t0.0\n",
      "0.8798684210526316\t0.5563157894736842\t0.5260526315789473\t0.6967105263157894\t0.9923684210526316\n"
     ]
    }
   ],
   "source": [
    "for acc_row in all_acc:\n",
    "    print(\"\\t\".join(map(str, acc_row)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lamol",
   "language": "python",
   "name": "lamol"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
