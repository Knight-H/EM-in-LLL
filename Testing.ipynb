{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b432e95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_transformers import AdamW, WarmupLinearSchedule, WarmupConstantSchedule\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import argparse\n",
    "import copy\n",
    "import logging\n",
    "import numpy as np\n",
    "import os, time, csv\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import higher\n",
    "from settings import parse_test_args, model_classes, init_logging\n",
    "from utils import TextClassificationDataset, dynamic_collate_fn, prepare_inputs, DynamicBatchSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18ecb72",
   "metadata": {},
   "source": [
    "# From Meta-MbPA Paper\n",
    "\n",
    ">In our experiments, we find that it is sufficient to take this to the extreme such that we consider all test examples as a single cluster. Consequently, we consider the whole memory as neighbours and we randomly sample from it to be comparable with the original local adaptation for- mulation (i.e. same batch sizes and gradient steps). As shown in the next section, it has two benefits: (1) it is more robust to negative transfer, (2) it is faster when we evaluate testing examples as a group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0b9e285",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0d70274",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dotdict({\n",
    "#     \"output_dir\": \"/data/model_runs/em_in_lll/MetaMBPA_order1\",\n",
    "    \"output_dir\": \"/data/model_runs/em_in_lll/MetaMBPA_order1_v2\",\n",
    "    \"adapt_lambda\": 1e-3,\n",
    "    \"adapt_lr\": 2e-3,\n",
    "    \"adapt_steps\": 20,\n",
    "    \"no_fp16_test\": False\n",
    "})\n",
    "output_dir = args[\"output_dir\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aaf46c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = pickle.load(open(os.path.join(output_dir, 'train_args'), 'rb'))\n",
    "args.update(train_args.__dict__)\n",
    "str(args)\n",
    "model_type = args[\"model_type\"]\n",
    "model_name = args[\"model_name\"]\n",
    "n_labels = args[\"n_labels\"]\n",
    "tasks = args[\"tasks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4273c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_class, model_class, args[\"tokenizer_class\"] = model_classes[model_type]\n",
    "tokenizer = args[\"tokenizer_class\"].from_pretrained(model_name)\n",
    "model_config = config_class.from_pretrained(model_name, num_labels=n_labels, hidden_dropout_prob=0, attention_probs_dropout_prob=0)\n",
    "save_model_path = os.path.join(output_dir, f'checkpoint-{len(tasks)-1}')\n",
    "model = model_class.from_pretrained(save_model_path, config=model_config).cuda()\n",
    "memory = pickle.load(open(os.path.join(output_dir, f'memory-{len(tasks)-1}'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15db9f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_task(task_id, args, model, test_dataset):\n",
    "\n",
    "    if not args.no_fp16_test:\n",
    "        model = model.half()\n",
    "\n",
    "    def update_metrics(loss, logits, cur_loss, cur_acc):\n",
    "        preds = np.argmax(logits, axis=1)\n",
    "        return cur_loss + loss, cur_acc + np.sum(preds == labels.detach().cpu().numpy())\n",
    "    \n",
    "    # Before anything else, just sample randomly from memory!\n",
    "    # Use this as sample going forward!\n",
    "    s_input_ids, s_masks, s_labels = memory.sample(32)\n",
    "    print(f\"Total No.# of sampled: {len(s_labels)}\")\n",
    "    # query like this first just like training... this will need to be removed later!!! so we can adapt 32x32\n",
    "    with torch.no_grad():\n",
    "        q_input_ids, q_masks, q_labels = memory.query(s_input_ids, s_masks)\n",
    "    \n",
    "    # Meta-Learning Local Adaptation\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    inner_optimizer = optim.SGD(optimizer_grouped_parameters, lr=args.learning_rate)\n",
    "    with higher.innerloop_ctx(model, inner_optimizer, copy_initial_weights=False,track_higher_grads=False) as (fmodel, diffopt):\n",
    "        # 1 Inner Loop (Support Set) - Once for all testing samples\n",
    "        for sup_input_ids, sup_masks, sup_labels in zip(q_input_ids, q_masks, q_labels):\n",
    "            sup_input_ids = sup_input_ids.cuda()\n",
    "            sup_masks = sup_masks.cuda()\n",
    "            sup_labels = sup_labels.cuda()\n",
    "            loss = fmodel(input_ids=sup_input_ids, attention_mask=sup_masks, labels=sup_labels)[0]\n",
    "            diffopt.step(loss)\n",
    "            \n",
    "        # 2 Outer Loop (Query Set)\n",
    "        tot_n_inputs = 0\n",
    "        cur_loss, cur_acc = 0, 0\n",
    "        all_labels, all_label_confs, all_preds = [], [], []\n",
    "        \n",
    "        test_dataloader = DataLoader(test_dataset, num_workers=args.n_workers, collate_fn=dynamic_collate_fn,\n",
    "                                     batch_sampler=DynamicBatchSampler(test_dataset, args.batch_size * 4))\n",
    "\n",
    "        for step, batch in enumerate(test_dataloader):\n",
    "            n_inputs, input_ids, masks, labels = prepare_inputs(batch)\n",
    "            tot_n_inputs += n_inputs\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                fmodel.eval()\n",
    "                output = fmodel(input_ids=input_ids, attention_mask=masks, labels=labels)[:2]\n",
    "                loss = output[0].item()\n",
    "                logits = output[1].detach().cpu().numpy()\n",
    "                softmax = F.softmax(output[1], -1) \n",
    "                \n",
    "            # Output has size of torch.Size([16, 33]) [BATCH, CLASSES]\n",
    "            label_conf = softmax[np.arange(len(softmax)), labels] # Select labels in the softmax of 33 classes\n",
    "            preds = np.argmax(logits, axis=1)\n",
    "\n",
    "            cur_loss, cur_acc = update_metrics(loss*n_inputs, logits, cur_loss, cur_acc)\n",
    "\n",
    "            # Append all!\n",
    "            all_labels.extend(labels.tolist())\n",
    "            all_label_confs.extend(label_conf.tolist())\n",
    "            all_preds.extend(preds.tolist())\n",
    "\n",
    "            if (step+1) % args.logging_steps == 0:\n",
    "                print(\"Tested {}/{} examples, test loss: {:.3f} , test acc: {:.3f}\".format(\n",
    "                    tot_n_inputs, len(test_dataset), cur_loss/tot_n_inputs, cur_acc/tot_n_inputs))\n",
    "\n",
    "\n",
    "    print(\"test loss: {:.3f} , test acc: {:.3f}\".format(\n",
    "        cur_loss / len(test_dataset), cur_acc / len(test_dataset)))\n",
    "    return cur_acc / len(test_dataset), all_labels, all_label_confs, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9b4f8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start testing yelp_review_full_csv...\n",
      "Total No.# of sampled: 32\n",
      "test loss: 8.006 , test acc: 0.004\n",
      "Start testing ag_news_csv...\n",
      "Total No.# of sampled: 32\n",
      "test loss: 6.987 , test acc: 0.000\n",
      "Start testing dbpedia_csv...\n",
      "Total No.# of sampled: 32\n",
      "test loss: 1.789 , test acc: 0.550\n",
      "Start testing amazon_review_full_csv...\n",
      "Total No.# of sampled: 32\n",
      "test loss: 7.823 , test acc: 0.003\n",
      "Start testing yahoo_answers_csv...\n",
      "Total No.# of sampled: 32\n",
      "test loss: 0.798 , test acc: 0.747\n"
     ]
    }
   ],
   "source": [
    "avg_acc = 0\n",
    "accuracies = []\n",
    "data_for_visual = []\n",
    "for task_id, task in enumerate(tasks):\n",
    "    print(\"Start testing {}...\".format(task))\n",
    "    test_dataset = TextClassificationDataset(task, \"test\", args, tokenizer)\n",
    "    task_acc, all_labels, all_label_confs, all_preds = test_task(task_id, args, model, test_dataset)\n",
    "\n",
    "    # Start Edit\n",
    "    data_ids = [task + str(i) for i in range(len(all_labels))]\n",
    "    data_for_visual.extend(list(zip(data_ids, all_labels, all_label_confs, all_preds)))\n",
    "    accuracies.append(task_acc)\n",
    "\n",
    "    avg_acc += task_acc / len(args.tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49f711d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.26092105263157894\n",
      "Accuracies: [0.003947368421052632, 0.0, 0.5502631578947368, 0.0030263157894736843, 0.7473684210526316]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average Accuracy: {avg_acc}\")\n",
    "print(f\"Accuracies: {accuracies}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lamol",
   "language": "python",
   "name": "lamol"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
